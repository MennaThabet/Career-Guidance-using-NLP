{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Patch Experta for Python 3.10+ Compatibility\n",
        "import collections\n",
        "import collections.abc\n",
        "collections.Mapping = collections.abc.Mapping"
      ],
      "metadata": {
        "id": "W6UAPtUhUjxD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (run only once in Colab)\n",
        "!pip install experta spacy nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQhFjd97Ux-H",
        "outputId": "94602f1f-6164-45fe-f0e8-497b590a298c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: experta in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: frozendict==1.2 in /usr/local/lib/python3.11/dist-packages (from experta) (1.2)\n",
            "Requirement already satisfied: schema==0.6.7 in /usr/local/lib/python3.11/dist-packages (from experta) (0.6.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import difflib\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from experta import Fact, Rule, DefFacts, KnowledgeEngine, MATCH\n"
      ],
      "metadata": {
        "id": "kYXNCDG3U2Yp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK Data Initialization\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUJNnL3QU5US",
        "outputId": "2b4e35a2-3790-4901-f0c5-bf67f29c2d77"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------- Load & Clean Data ----------\n"
      ],
      "metadata": {
        "id": "wIu8O4dfcyEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/Career Guidance Expert System.csv\"\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "AKUEpPV8U9B6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty lists with NaN for handling\n",
        "data.replace({\"[]\": np.nan}, inplace=True)"
      ],
      "metadata": {
        "id": "pV_OsOj7cAkC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert skill strings to actual lists\n",
        "def to_list(entry):\n",
        "    try:\n",
        "        return [item.strip().lower() for item in ast.literal_eval(entry)]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "data[\"hard_skill\"] = data[\"hard_skill\"].apply(to_list)\n",
        "data[\"soft_skill\"] = data[\"soft_skill\"].apply(to_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "R_L3jBRxVEFE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing skills with mode values for each field\n",
        "def impute_missing_skills(df):\n",
        "    hard_mode = df.groupby(\"candidate_field\")[\"hard_skill\"].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else [])\n",
        "    soft_mode = df.groupby(\"candidate_field\")[\"soft_skill\"].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else [])\n",
        "\n",
        "    def fill(row):\n",
        "        if not row[\"hard_skill\"]:\n",
        "            row[\"hard_skill\"] = hard_mode.get(row[\"candidate_field\"], [])\n",
        "        if not row[\"soft_skill\"]:\n",
        "            row[\"soft_skill\"] = soft_mode.get(row[\"candidate_field\"], [])\n",
        "        return row\n",
        "\n",
        "    return df.apply(fill, axis=1)\n",
        "\n",
        "data = impute_missing_skills(data)"
      ],
      "metadata": {
        "id": "EsIRWgujVHew"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------- Knowledge Base Construction ----------\n"
      ],
      "metadata": {
        "id": "tgkfYy_TcZUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a skill map from dataset\n",
        "career_map = {\n",
        "    field: {\n",
        "        \"hard\": set(sum(group[\"hard_skill\"].tolist(), [])),\n",
        "        \"soft\": set(sum(group[\"soft_skill\"].tolist(), []))\n",
        "    }\n",
        "    for field, group in data.groupby(\"candidate_field\")\n",
        "}"
      ],
      "metadata": {
        "id": "FQkU_be0VLRT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all known skills\n",
        "known_skills = sorted(set().union(*(entry[\"hard\"] | entry[\"soft\"] for entry in career_map.values())))"
      ],
      "metadata": {
        "id": "BobaSX38VPoQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------- NLP Initialization ----------"
      ],
      "metadata": {
        "id": "E89iydNUc1j4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure NLTK data is downloaded before use\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "CySSXmN0VV0E"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the extract_terms function\n",
        "def extract_terms(text):\n",
        "    \"\"\"Tokenizes, removes stop words, and lemmatizes the input text.\"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.text for token in doc if token.text not in string.punctuation]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "91aDMgc7ery7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------- Expert System using Experta ----------\n"
      ],
      "metadata": {
        "id": "3x1okj_KdR6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Profile(Fact):\n",
        "    \"\"\"User-provided skill description\"\"\"\n",
        "    pass\n",
        "\n",
        "class Advisor(KnowledgeEngine):\n",
        "\n",
        "    @DefFacts()\n",
        "    def _init_facts(self):\n",
        "        yield Fact(intent=\"analyze\")\n",
        "\n",
        "    @Rule(Fact(intent=\"analyze\"), Profile(description=MATCH.description))\n",
        "    def suggest_career(self, description):\n",
        "        tokens = extract_terms(description)\n",
        "        matched = []\n",
        "\n",
        "        for token in tokens:\n",
        "            best_match, best_score = None, 0\n",
        "            for reference in known_skills:\n",
        "                score = difflib.SequenceMatcher(None, token, reference).ratio()\n",
        "                if score > best_score:\n",
        "                    best_match, best_score = reference, score\n",
        "            if best_score >= 0.6:\n",
        "                matched.append(best_match)\n",
        "\n",
        "        ranking = {\n",
        "            field: sum(skill in skills[\"hard\"] for skill in matched) +\n",
        "                   sum(skill in skills[\"soft\"] for skill in matched)\n",
        "            for field, skills in career_map.items()\n",
        "        }\n",
        "\n",
        "        if any(ranking.values()):\n",
        "            recommendation = max(ranking, key=ranking.get)\n",
        "            print(f\"\\n✅ Recommended Career Field: **{recommendation.title()}**\")\n",
        "        else:\n",
        "            print(\"\\n⚠️ No suitable match found. Please revise your input.\")\n"
      ],
      "metadata": {
        "id": "u_60Um_pVYjF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---------- Run System ----------"
      ],
      "metadata": {
        "id": "vtg9N0tfdcnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input and run the expert system\n",
        "user_input = input(\"🧠 Describe your experience, skills, or projects:\\n\")\n",
        "\n",
        "if user_input.strip():\n",
        "    expert = Advisor()\n",
        "    expert.reset()\n",
        "    expert.declare(Profile(description=user_input))\n",
        "    expert.run()\n",
        "else:\n",
        "    print(\"⚠️ Empty input. Please try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl9U-RbiODqk",
        "outputId": "ed2a4ac3-ff47-4d78-ec7d-98e8677e304d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Describe your experience, skills, or projects:\n",
            " \"I have experience in nursing, registration,  and service. I am also good at written communication.\"\n",
            "\n",
            "✅ Recommended Career Field: **Healthcare & Medical**\n"
          ]
        }
      ]
    }
  ]
}